[home](./index.md)
-------------------

*author: niplav, created: 2019-05-22, modified: 2019-08-15, language: english, status: in progress, importance: 3, confidence: unlikely*

> __Short texts on different topics.__

Notes
=====

Getting an Overview Over Everything
-----------------------------------

Many people want to learn everything ([Drexler
2009](http://metamodern.com/2009/05/27/how-to-learn-about-everything/),
[Young
2008](https://www.scotthyoung.com/blog/2008/06/12/the-goal-of-learning-everything/)).
This poses a significant challenge, and begins with the problem of
figuring out what “everything” is supposed to contain.

One possible method one could use to get
an overview of everything is to use [Wikipedia's
contents:Outlines](https://en.wikipedia.org/wiki/Portal:Contents/Outlines):
it contains a list of all
[outlines](https://en.wikipedia.org/wiki/Outline_(list)) on Wikipedia,
and is well structured. Wikipedia is generally concise and complete
enough to provide a sufficient overview over a topic (see [Tomasik
2017](https://foundational-research.org/education-matters-for-altruism/#Taking_the_big_picture_Textbooks_review_articles_and_Wikipedia)).
To read this collection of outlines completely, one could use the
following method:

Read [Wikipedia's
contents:Outlines](https://en.wikipedia.org/wiki/Portal:Contents/Outlines)
from top to bottom. If a link is a link to a Wikipedia article, open it
and read it, without opening any further links. If a link leads to an
outline, open the link and recursively apply the same procedure to the
outline. If an article is opened a second time, it can be discarded. Lists
can also be ignored.

This method results in a corpus of considerable size.<!--(TODO: how much text?)-->

Approach Anxiety
----------------

I had very big problems when trying to overcome approach anxiety. Since
this seems to be a common problem, it may be worth it to share a method
that was successful for me to become more comfortable with it. To make
progress on that front, I set a goal for several weeks in advance,
similar to this:

* 1st week: Ask 1 guy a day for his number
* 2nd week: Ask 2 guys for their numbers
* 3rd week: Ask 3 guys for their numbers
* 4th week: Ask 3 guys and 1 girl for their numbers
* 5th week: Ask 2 guys and 1 girl for their numbers
* 6th week: Ask 1 guy and 2 girls for their numbers
* 7th week: Ask 3 girls for their numbers

If I failed in a week, I precommitted to donating a certain amount like
5€ to an effective charity.

In contrast to the other methods used and advertised by people who cold
approach, this worked quite well. After the first 4 weeks, I took 1 week
off, but went back to approaching the week after.

This, of course, only can work if one is able to ask men for their
numbers. For a reason that eludes me, this is easier for me than
approaching women, though the gap is shrinking with exposure.

Converging Preference Utilitarianism
------------------------------------

One problem with [preference
utilitarianism](https://en.wikipedia.org/wiki/Preference_utilitarianism)
is the difficulty of aggregating and comparing preferences
interpersonally, as well as a critique that some persons have very
altruistic and others very egoistic preferences.

### Method

A possible method of trying to resolve this is to try to hypothetically
calculate the aggregate preferences of all persons in the following
way: For every existing person pₐ, this person learns about the
preferences of all other persons pₙ. For each pₙ, pₐ learns about
their preferences and experiences pₙ's past sensory inputs. pₐ then
updates their preferences according to this information. This process
is repeated until the maximal difference between preferences has shrunk
to a certain threshold.

### Variations

One possible variation in the procedure is between retaining knowledge
about the identity of pₐ, the person aggregating the preferences. If
this were not done, the result would be very akin to the [Rawlsian Veil
of Ignorance](https://en.wikipedia.org/wiki/Veil_of_ignorance#Rawls'_version).

Another possible variation could be not attempting to achieve convergence,
but only simply iterating the method for a finite amount of times. Since
it's not clear that more iterations would contribute towards further
convergence, maybe 1 iteration is desirable.

### Problems

This method has a lot of ethical and practical problems.

#### Assumptions

The method assumes a bunch of practical and theoretical premises,
for example that preferences would necessarily converge upon
experiencing and knowing other persons qualia and preferences.
It also assumes that it is in principle possible to make a person
experience other persons qualia.

#### Sentient Simulations

Since each negative experience would be experienced by every
person at least one time, and negative experiences could
considered to have negative value, calculating the converging
preferences would be unethical in practice (just as [simulating the
experience](https://foundational-research.org/risks-of-astronomical-future-suffering/#Sentient_simulations)
over and over).

#### Genuinely Selfish Agents

If an agent is genuinely selfish (has no explicit term for the welfare of
another agent in its preferences), it might not adjust its own preferences
upon experiencing other lifes. It might even be able to circumvent the
veil of ignorance to locate itself.

#### Lacking Brain Power

Some agents might lack the intelligence to process all the information
other agents perceive. For example, an ant would probably not be able
to understand the importance humans give to art.

### See Also

* [Yudkowsky 2004](./doc/converging_preference_utilitarianism/coherent_extrapolated_volition_yudkowsky_2004.pdf "“Coherent Extrapolated Volition” (Eliezer Yudkowsky, 2004)")

Silent & Loud Killers
---------------------

The idea of a [Great Filter](https://en.wikipedia.org/wiki/Great_Filter)
(see also [Hanson 1998](http://mason.gmu.edu/~rhanson/greatfilter.html)
proposes that we do not observe aliens because in the development of
intelligent life, there is one or more obstacles that obliterate the
developing societies before they can start to colonize their own galaxy.

One big question that poses itself is whether humanity is
before or after such a filter. Some examples of potential
filters that still await humanity are named in [Bostrom
2008](./doc/silent_killers/where_are_they_bostrom_2008.pdf "“Where
Are They?” (Nick Bostrom, 2008)"):

> We can identify a number of potential existential risks: nuclear
> war fought with stockpiles much greater than those that exist today
> (maybe resulting from future arms races); a genetically engineered
> superbug; environmental disaster; asteroid impact; wars or terrorists act
> committed with powerful future weapons, perhaps based on advanced forms
> of nanotechnology; superintelligent general artificial intelligence with
> destructive goals; high‐energy physics experiments; a permanent global
> Brave‐New‐World‐like totalitarian regime protected from revolution
> by new surveillance and mind control technologies. These are just some
> of the existential risks that have been discussed in the literature, and
> considering that many of these have been conceptualized only in recent
> decades, it is plausible to assume that there are further existential
> risks that we have not yet thought of.

*– [Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom), [“Where Are They”](./doc/notes/where_are_they_bostrom_2008.pdf) p. 7, 2008*

These risks can be categorized into two groups: silent killers and
loud killers. A loud killer is an existential catastrophe that produces
astronomical amounts of energy and with that light. Such an event would be
visible from earth if it occurred in our galaxy. Examples for loud killers
would be superintelligent artificial intelligence (maximizing its utility
function by expanding at a appreciable fraction of the speed of light),
high-energy physics experiments (although there are exceptions, such as
creating black holes), and perhaps failure from advanced nanotechnology
(also expanding rapidly). A silent killer represents the counterfactual
case: An existential catastrophe that doesn't produce astronomical
amounts of energy and light. This includes pandemics, environmental
disaster and totalitarian regimes.

Some failure modes do not fall clearly into either of these categories.
Examples are nuclear wars and terrorist acts with powerful weapons,
since these can have a wide variation in intensity.

If humanity is before a Great Filter, it seems likely that this filter
is not a loud killer, since many civilizations will have encountered the
same catastrophe, but we do not observe any such irregular phenomena
when examining the universe. This is presumably good news, since it
restricts the amount of possible filters still ahead of us.
