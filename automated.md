[home](./index.md)
------------------

*author: niplav, created: 2023-07-03, modified: 2024-04-16, language: english, status: finished, importance: 3, confidence: log*

> __An unstructured, unfinished braindump.__

Automated AI Alignment Research
================================

### I. Type Signature of Output

When we create scaffolds for/train/create training environments for
automated AI alignment researchers, what is the type signature of the
outputs of those researchers?

* Model weights of new, presumably aligned, more powerful AI systems?
* New architectures for AI systems?
* Proofs of convergence or OOD generalization of new architectures?
* Enumerative safety through mechanistic interpretability?
	* That seems hard, there are probably exponentially many meaningful circuits in large neural networks
* New AI paradims that side-step inner optimizers?
	* Neo-GOFAI?
	* Infra-Bayesian physicalism implementations?
* Control techniques that allow for better supervision of the next generation of automated alignment researchers?
* Just ask the automated alignment researchers what the type signature should be?

### II. Condition Number of Succession Process

### III. Generator-Verifier Gap Broken/Unusual For Alignment

There are different perspectives on this.

### IV. Goal-Guarding + Adversarial Examples
