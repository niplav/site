[home](./index.md)
------------------

*author: niplav, created: 2023-07-06, modified: 2023-07-26, language: english, status: in progress, importance: 5, confidence: probable*

> __Hi Anders__

Comments on “Grand Futures”
============================

### 0.1

> The aim is to develop solid arguments about these
questions, such as bounds on how much computation could be done
given up-to-date physical theory or game-theoretic arguments
that civilizations meeting certain assumptions would behave in
certain ways when they met.

Perhaps change to:

> The aim is to develop solid arguments about these questions, such as
bounds on how much computation could be done given up-to-date physical
theory__,__ or game-theoretic arguments that civilizations meeting
certain assumptions would behave in certain ways __if__ they met.

---

> Performing more than a __google__ (10<sup>100</sup>) computations

I think it is supposed to be
"__[googol](https://en.wikipedia.org/wiki/Googol)__" instead of "google".

> Ord and Cotton-Barrett defined “An existential eucatastrophe is an
event which causes there to be much more expected value after the event
than before.” [684]²

The citation 684 is for "Gravitationally redshifted absorption lines in
the x-ray burst spectra of a neutron star", but I think it should be 685,
"Existential risk and existential hope: definitions". This off-by-one
pattern repeats itself throughout my copy of the book.

### 0.3

> The usage of the term civilization in this book needs to be
disambiguated.  Civilization can be used to denote complex societies
with features such as irrigation, urban areas, social stratification,
symbolic systems of communication, division of labour etc.

Perhaps use quotation marks around "civilization"
here—this feels like a [mention more than a
use](https://en.wikipedia.org/wiki/Use—mention_distinction).

> I will be using the term civilization not in the narrow high culture
sense but to denote __as__ a cohesive, long-range (social) structure
with a high degree of coordination

Same with quotation here, also the "as" feels stumbly.

### 0.4

> Near-term is more predictable and hence conclusions are more likely
to be true

I don't know of a good resource that firmly establishes this. [Dillon
2020](https://rethinkpriorities.org/publications/data-on-forecasting-accuracy-across-different-time-horizons)
and my [own investigation](./range_and_forecasting_accuracy.html)
don't come to firm conclusions, mostly because of
a dearth of good long-term forecasting data. [Muehlhauser
2019](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting)
also laments the lack of long-term forecasts that are precise enough to
be checked. I think later in the book there will be talk about why less
accurate forecasts should be expected in dynamical systems, and I look
forward to that discussion.

I haven't yet done a full review of that field yet, and my guess is that
there are much better references out there (maybe even on my hard drive).

#### 1.1.4

> While this book certain__ty__ is not intended as science fiction, it
may well find use as a source of raw material for science fiction authors.

Typo, should be "certain__ly__".

> [Stanislaw Lem — Summa Technologiae, comments on prognostication]

There is a work by Lem which I have only been able to find references
to in German. It's called "Das Katastrophenprinzip: Die Welt as
Holocaust" (Stanisław Lem, 1983). It gives an interesting perspective
on predictability, stating that the universe is *less* predictable in
the short term than in the long term. This feels related to notions of
sophistication from complexity theory, which try to capture the property
of being complex but not random.

<!--Read & excerpt it, and refer Sandberg to it-->

#### 1.1.5

> But for having a liveable society with an open future dynamism looks
better.

This sentence tripped me up, twice.

#### 1.2.1

In terms of macrohistory, one (self-described) amateur attempt is
[Muehlhauser
2017a](http://lukemuehlhauser.com/three-wild-speculations-from-amateur-quantitative-macrohistory/ "Three wild speculations from amateur quantitative macrohistory")
and [Muehlhauser
2017b](http://lukemuehlhauser.com/industrial-revolution/ "How big a deal was the Industrial Revolution?"),
which find that along 5 different metrics, human well-being has been
improving increasingly rapidly since the industrial revolution.

Also relevant to 1.2.2.

> A pessimistic possibility is that the set of people skilled in core
competences is so small compared to the overall population today that many
key technologies would be lost in a fall of civilization. An optimistic
counterargument may be that in this case many flock to the frarmer or
smith to learn their skills.

For an investigation into such global collapse at
current civilizational levels causing human extinction, see [Rodriguez
2020](https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would).
It broadly finds that recovery appears likely in scenarios where less
than 99.9% of all humans die, and finds extinction unlikely unless more
than 99.999% of all humans die.

#### 1.2.3

> although underground storage aims at storage for `$1 \times 10^5 yr$`

Word storage is repeated, perhaps use another term? Maybe use "safety for"
instead of "storage for".

> *La tombe du soldat inconnu* in Paris has been guarded and with a lit
eternal flame since 1920.

There is a large amount of ["eternal"
flames](https://en.wikipedia.org/wiki/Eternal_flame) around the world. I
don't know of any dataset that estimates the age of such eternal flames,
and their failure rates.

<!--
> An oft recounted tale that an Oxford college needed timber to repair
the great hall and found that it owned a forest planted 500 years ago
for that very purpose appears to be merely a good tale.

TODO: find out whether similar Notre-Dame rumor is true.

<https://www.architecturaldigest.com/story/two-years-later-heres-latest-notre-dames-restoration/amp>
-->

#### 1.2.4

> This is not merely large constructions such a Bering Strait bridge or
a trans-oceanic subway *in* the world

Shouldn't it be "such *as* a Bering Strait bridge"?

#### 1.2.5

> I.__J__. Rabi

Should be 'I.I. Rabi'.

#### 1.2.6

> It dealt with not just eugenics ([…]) but that human evolution
would become a result of cultural or political decisions.

The part after the parentheses might benefit from two or three more words,
"but also with the hypothesis that […]".

#### 1.3.1

> and be compact enough for a rapid exchange__d__ of in__-__ formation
among them.

Two typos (I think), I guess it should be "a rapid exchange of information
among them."

### 1.5

> More branches?

Perhaps elaborate on distinction between
transhumanism/posthumanism/extropianism, somewhere.

### 2.1

I like `$r$` more for radial distances.

### 3.4

> Loss of biodiversity might be irreversible and regrettable, yet that
permanent loss might not mean a permanent loss of potential.

I personally *do* feel a sense of loss at languages, species,
cultures & ecosystems going extinct. (I think it's sad trilobites
& opabinia regalis with its five eyes are gone, and would like to
revive them, even though this conflicts with wild-animal suffering
concerns.) Similarly, projections that [by 2050 >90% of all languages
will have gone extinct](http://dx.doi.org/10.1126/science.1096546)
seem like a tragedy to me, and I like the fact that there are (some)
language revitalization and [language](https://en.wikipedia.org/wiki/FirstVoices) [documentation](https://en.wikipedia.org/wiki/OLAC) projects underway.

#### 3.4.2

> many, many worlds worlds

Typo

#### 3.4.3

> Finding the right priorization strategy is non-trivial. One heuristic
is nick Bostrom's MaxiPOK principle “ “maximize the chance of an
OK outcome”.

Typo: after "principle" there are two quotation marks.

> [Other strategies]
> [Priority. Targeted vs broad interventions. Diff tech
development. Maxipok. Global public good. Safety beats haste.]

MaxiPOK was already mentioned & explained. For differential development
there's also differential intellectual progress, as explained in
“Differential Intellectual Progress as a Positive-Sum Project”
(Brian Tomasik, 2017).

> [?, 232]
[Trajectory changes. Planning?]

First reference is broken.

###### “Stupidity”: cognitive failures of coordination

> If the total amount of work done is proportional to the number of
members and the fraction of time not spent on meetings, then it is
`$T_{work}=c_1N(1-c_2 N)$`.

What do `$c_1$` and `$c_2$` stand for?

#### 3.5.3

> but hard for e.g. space when not easy enforcement

Now I'm wondering how spacetime ownership might look like, especially
around black holes (and how that spacetime might be taxed in a Georgist
fashion).

##### The singleton and cybernetics

> Even one implemented as an autonomous AGI can have a corrigible value
function that it does not resist changing [2700, 2218].

Corrigibility remains (afaik) fundamentally unsolved in
the toy cases (at least one that is demanded in [Soares et al.
2015](./doc/cs/ai/alignment/corrigibility/corrigibility_soares_et_al_2015.pdf),
so perhaps a "probably" or "may" would be good here.

### 4.1

> **However**, if these differences can be cached out in normatively
relevant unfairness, political power, or social scarce resourcse such
as status (see **box** ), then this **may** be an issue that **may**
motivate distributive actions. **However**, redistributing goods may
be less important than (if possible) influencing the fairness, power
or status.

(Emphases mine)

Some quibbles: "However" is written twice, as is "may" in the first
sentence. And there is a stray space after "box".

##### Artificial scarcity and coordination limits

> badly made poetry

I like the image of someone haphazardly slapping together some poetry
with their clumsy hands.

#### 4.2.1

##### EROI

Details 15 ("Space solar EROI"), footnote a: what does `$S$` stand for?

##### Nuclear energy

Footnote 30 links to footnote 31, which links…nowhere?

##### Heat dissipation in the environment

> A civilization that wants to use more energy on Earth without heating it
up __need__ to increase the emissivity or albedo throguh surface changes³²
or geoengineering, or add an external cooling system for more extreme uses

"needs" would read more smoothly, methinks.

#### 4.3.1 Raw materials

> If `$M(r) \propto r^{-α} α > 1$`

I think there should be a comma between `$r^{-α}$` and `$α > 1$`.

#### 4.4.1 Productivity limits

> and its logarithm __the the__ learning coefficient

#### 4.4.2 Automation

##### Artificial intelligence

> This is especially true since once the system has learned the task it
might be possible to copy its performance (having it act as an ANI or
generate an ANI with the skill`$^{51}$`).

Some of the literature on distilling larger neural networks into smaller
ones might be relevant here, but I haven't read any of it.<!--TODO:
read-->

##### Safety and control

> People recognizes that “life will find a way”

Should be "recognize"

##### Limits to self replication

> Perhaps somewhat surprising there are no quantum self-replicators. The
no cloning theorem means that copying arbitrary quantum states
is impossible. A corollary is that there is no universal quantum
constructor since it would be able to clone itself, a “no self
replication theorem”.

Probably should start with "Perhaps somewhat surprising__ly__".

If the no cloning theorem says that *arbitrary* quantum states
can't be copied, can't there be a subspace of quantum states that is
capable of replication, similar to how arbitrary hard SAT problems are
computationally intractable but in practice we can solve real-world
instances just fine?

Or is the issues with the *universal* quantum constructor, since such
a replicator wouldn't be able to create arbitrary quantum states?

#### 4.5.2 Autopoiesis

##### Autopoietic societies

> and New Zealand was likely settled a founder population with between
50 and 100 women

Missing "by": "settled __by__ a founder population with […]"

### 4.6 Services

> It can be argued that the heterogeneity of services requires human-level
artificial intelligence, but a vast number of services can be performed
by more narrow systems adequately (making the human-provided version a
valuable luxury in many cases: small talk with the hairdresser may soon
be the actual reason to pay the premium).

I somewhat balk at the specific example, as the job of a hair dresser
(from a robotic perspective, though I'm no roboticist) seems to me
to be one of the hardest professions to automate (hair is non-rigid
& highly non-linear & malfunctioning machinery would be dangerously
close to a human neck, and the wages aren't high enough for there to
be significant pressure for automation). I'd maybe pick legal advice &
a charismatic lawyer instead, or medical advice and empathy (though both
of these examples suffer from the fact that these two professions have
far more legal protection).
