[home](./index.md)
------------------

*author: niplav, created: 2021-03-31, modified: 2021-04-11, language: english, status: notes, importance: 3, confidence: highly unlikely*

> __This page contains my notes on ethics, separated from my regular
notes to retain some structure to the notes.__

Notes on Ethics
================

My general ethical outlook is one of high [moral
uncertainty](./doc/ethics_notes/moral_uncertainty_macaskill_et_al_2020.pdf),
with my favourite theory being consequentialism. I furthermore favour
hedonic, negative-leaning, and act-based consequentialisms.

However, most notes on this page don't depend on these assumptions.

Note that while I am interested in ethics, I haven't read as much about
the topic as I would like. This probably leads to me re-inventing a large
amount of jargon, and making well-known (and already refuted) arguments.

Humans Implement Ethics Discovery
----------------------------------

Humans sometimes change their minds about what they consider to be good,
both on a individual and on a collective scale. One obvious example is
slavery in western countries: although our wealth would make us more
prone to admitting slavery (high difference between wages & costs of
keeping slaves alive), we have nearly no slaves. This used to be different,
in the 18th and 19th century, slavery was a common practice.

This process seems to come partially from learning new facts about
the world (e.g., which ethical patients respond to noxious stimuli,
how different ethical patients/agents are biologically related to each
other, etc.), let's call this the *model-updating process*. But there also
seems to be an aspect of humans genuinely re-weighting their values when
they receive new information, which could be called the *value-updating
process*. There also seems to be a third value-related process
happening, which is more concerned with determining inconsistencies
within ethical theories by applying them in thought-experiments (e.g. by
discovering problems in population axiology, see for example [Parfit
1986](./doc/ethics_notes/overpopulation_and_the_quality_of_life_parfit_1986.pdf "Overpopulation and the Quality of Life")).
This process might be called the *value-inference process*.

One could say that humans implement the *value-updating*
and the *value-inference* process – when they think about
ethics, there is an underlying algorithm that weighs trade-offs,
considers points for and against specific details in theories,
and searches for maxima. As far as I know, there is no crisp
formalization of this process (initial attempts are [reflective
equilibrium](https://plato.stanford.edu/entries/reflective-equilibrium/)
and [coherent extrapolated
volition](./doc/converging_preference_utilitarianism/coherent_extrapolated_volition_yudkowsky_2004.pdf)).

If we accept the [complexity of human
values](https://arbital.com/p/complexity_of_value/) hypothesis, this
absence of a crisp formalism is not surprising: the algorithm for
*value-updating* and *value-inference* is probably too complex to
write down.

However, since we know that humans are existing implementations of this
process, we're not completely out of luck: if we can preserve humans
"as they are" (and many of the notes on this page try to get at what
this fuzzy notion of "as they are" would mean), we have a way to further
update and infer values.

This view emphasizes several conclusions: preserving humans "as they
currently are" becomes very important, perhaps even to the extent of
misallowing self-modification, the loss of human cultural artifacts
(literature, languages, art) becomes more of a tragedy than before
(potential loss of information about what human values are), and making
irreversible decisions becomes worse than before.

<!--Often, change in values seems forseeable. Why? How?-->

I Care About Ethical Decision Procedures
-----------------------------------------

Or, why virtue ethics alone feels misguided.

In general, ethical theories want to describe what is good and what
is bad. Some ethical theories also provide a decision-procedure: what
to do in which situations. One can then differentiate between ethical
theories that give recommendations for action in every possible situation
(we might call those *complete theories*), and ethical theories that
give recommendations for action in a subset of all possible situations
(one might name these *incomplete theories*, although the name might be
considered unfair by proponents of such theories).

<!--Add stuff about partial orderings of actions, with multiple maximal elements?-->

It is important to clarify that incomplete theories are not necessarily
indifferent between different choices for action in situations they have
no result for, but that they just don't provide a recommendation for action.

Prima facie, complete theories seem more desirable than incomplete
theories – advice in the form of "you oughtn't be in this situation
in the first place" is not very helpful if you are confronted with such
a situation!

Virtue ethics strikes me as being such a theory – it defines what is
good, but provides no decision-procedure for acting in most situations.

At best, it could be interpreted as a method for developing such a
decision-procedure for each individual agent, recognizing that an attempt
at formalizing an ethical decision-procedure is a futile goal, and instead
focussing on the value-updating and value-inference process itself.

Deference-Attractors of Ethical Agents
-------------------------------------

Basically: When I'm angry or stressed, I would like to defer to
versions of myself that are relaxed & calm. If we draw a network of
these deferrings, there are probably some attractors.

### Deceptive Deference-Attractors

What Use Ethics For?
---------------------

Everyday life, or problems that arise in the limit?

C.f. High Energy Ethics.

The Two Urgent Problems are: How Don't We Die and How Do We Become Happy?
--------------------------------------------------------------------------

A Very Subjective Ranking of Types of Ethical Theories
-------------------------------------------------------

Consequentialisms, Contractualism, Deontology, Virtue Ethics

What Is Wrong With the Unwilling Organ-Donor Thought Experiment?
-----------------------------------------------------------------

Problems with game-theoretical ethical intuitions.

Better framing: Create universe, make decision, destroy universe after
payoff time.

For most people, there's a point where they kill the unwilling organ
donor, so we're basically haggling over the price. Maybe just a Sorites
paradox?

Why Death is Bad
-----------------

Under moral uncertainty with evolving preferences, you want to keep
options open, but death closes all options but one, potentially losing
a lot of future value.

In a sense, it's unfair towards all other ethical systems you embody
to kill yourself.

Possible Surprising Implications of Moral Uncertanity
------------------------------------------------------

Preserving languages & biospheres might be really important, if the
continuity of such processes is morally relevant.

We should try to be careful about self-modification, lest we fall into
a molochian attractor state we don't want to get out of. Leave a line
of retreat in ideology-space!

The Repugnant Conclusion Probably Implies Many or Only One Moral Patients
--------------------------------------------------------------------------

Diminishing or increasing returns on investments for well-being
of a single agent?

If we're really lucky, initially there are increasing returns, but at
some point they start diminishing.
