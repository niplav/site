[home](./index.md)
-------------------

*author: niplav, created: 2020-12-30, modified: 2021-01-08, language: english, status: in progress, importance: 3, confidence: opinion*

> __Reading a text is sometimes a big time investment, but people
don't approach their reading in a structured manner, e.g. by keeping
notes, making flashcards, doing exercises, writing reviews or making
summaries. I have been taking notes on the books I read since mid 2017,
but have neglected writing reviews or summaries that might be useful to
others. This is my attempt at salvaging that oversight.__

Text Reviews
=============

The Human Predicament (David Benatar, 2017)
--------------------------------------------

“The Human Predicament” is a book about life philosophy, written
by the pessimistic analytic philosopher David Benatar. In it, Benatar
describes what he calls the human predicament (hence the title), which
consists in the fact that human lives are usually bad, and much worse
than people themselves think. In his view, human lives lack cosmic (and
sometimes terrestrial) meaning, are bad because they're much shorter
than they could be, much more filled with pain and discomfort than
humans think, and full of ignorance, unfulfilled desires and physical
deterioration during the course of one's lifetime.

However, all alternatives are also bad: death, because it often deprives
of life, and annihilates the person dying; and suicide, for much the
same reasons, unless it annihilates a life that is awful enough to
justify death.

The only positive option is to not come into existence at all–or at
least not make others come into existence, even though one desires to.
He alludes several times to one of his other books, Better Never To Have
Been, in which he advocates for antinatalism.

Reading this book felt a little bit pointless to me. Since beliefs
are for action<!--TODO: gwern link-->, and Benatar is just applying a
linear transformation to all available options (if everything's bad,
nothing is), I didn't gain anything tangible from this text. I had a
phase where I believed antinatalism quite strongly, and still don't
plan on having kids (although I know that this attitude might change
with increasing age), but overall antinatalism does not strike me as
a pragmatic policy<!--cf. David Pearce & Brian Tomasik ("Strategic
Considerations for Ethical Anti-natalists")-->, me instead adopting an
anti-pure-replicator<!--TODO: link to Emilsson--> strategy.

Especially the chapter on meaning felt irrelevant: I don't have an
internal experience of meaning (or the lack thereof), and oscillate
between believing it to be a subtype of high-valence qualia and believing
it to be a mechanism for the mind to do things that are in themselves
not enjoyable (a "second reward signal" next to pleasure).

Benatar mentions cryonics, life extension technology and
transhumanism in general, and while his treatment of these topics
is more respectful than most, he dismisses them very quickly. I
disagree on these points, given that it seems that humanity
can expect to find itself in a period of [hyperbolic economic
growth](https://sideways-view.com/2017/10/04/hyperbolic-growth/ "Hyperbolic growth")
(see [Roodman
2020](https://www.openphilanthropy.org/blog/modeling-human-trajectory "Modeling the Human Trajectory")).

I am also not a fan of the pessimism-optimism distinction. Benatar himself
touches on this:

> that a view is pessimistic should, in itself, neither
count in its favor nor against it. (The same, of course, is true
of an optimistic view.)

*– David Benatar<!--TODO: link-->, “The Human Predicament” p. 225, 2017*

It seems to me that humans can believe very bad things and still be
happier than most in their lives (I know this is at least true for one
human, myself). This, combined with the fact that Benatar simply shifts
the utility function downwards, makes me prone to rejecting his worldview
as simply a matter of emotional tone on the same facts.

Finally, I want to accuse Benatar of insufficient pessimism (on his own
criteria): The most likely outcome for humanity (and for life in general)
seems not to be total extinction, but instead a universe filled with
beings most capable of copying themselves, the whole cosmos teeming with
subsistence-level beings with very boring conscious experiences<!--TODO:
hanson paper about frontiers & SSC post about substrate type stuff-->
until the stars go out. (Or even worse scenarios from anti-aligned
artificial intelligences).

Overall, the book had some interesting points about suicide, the quality of
life and meaning, but felt rather pointless.

__3/10__

What failure looks like (Paul Christiano, 2019)
-----------------------------------------------

*Written for the [2019 LessWrong Review](https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review)*

[Original Post](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)

I read [this
post](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
only half a year ago after seeing it being referenced in several different
places, mostly as a newer, better alternative to the existing FOOM-type
failure scenarios. I also didn't follow the comments on this post when
it came out.

This post makes a lot of sense in Christiano's worldview, where we have a
relatively continuous, somewhat multipolar takeoff which to a large extent
inherits the problem in our current world. This is especially applies to
part I: we already have many different instances of scenarios where humans
follow measured incentives and produce unintended outcomes. [Goodhart's
law](https://en.wikipedia.org/wiki/Goodhart%27s_law) is a thing. Part
I ties in especially well with Wei Dai's concern that

> AI-powered memetic warfare makes all humans effectively insane.

While I haven't done research on this, I have a medium
strength intuition that this is already happening. Many
people I know are at least somewhat addicted to the internet,
having lost a lot of attention due to having their motivational
system hijacked, which is worrying because [Attention is your scarcest
resource](https://www.lesswrong.com/posts/aDtzAZf3LnwYvmBP7/attention-is-your-scarcest-resource).
I believe investigating the amount to which attention has
deteriorated (or has been monopolized by different actors)
would be valuable, as well as thinking about which incentives
will start when AI technologies become more powerful ([Daniel
Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo) has been
writing especially interesting essays on this kind of problem).

As for part II, I'm a bit more skeptical. I would summarize "going
out with a bang" as a "collective treacherous turn", which would
demand somewhat high levels of coordination between agents of various
different levels of intelligence (agents would be incentivized to turn
early because of first-mover-advantages, but this would increase the
probability of humans doing something about it), as well as agents
knowing very early that they want to perform a treacherous turn to
influence-seeking behavior. I'd like to think about how the frequency of
premature treacherous turns relates to the intelligence of agents. Would
that be continuous or discontinuous? Unrelated to Christiano's post,
this seems like an important consideration (maybe work has gone into
this and I just haven't seen it yet).

Still, part II holds up pretty well, especially since we can expect AI
systems to cooperate effectively via merging utility functions, and we
can see systems in the real world that fail regularly, but not much is
being done about them (especially social structures that sort-of work).

I have referenced this post numerous times, mostly in connection with a
short explanation of how I think current attention-grabbing systems are
a variant of what is described in part I. I think it's pretty good, and
someone (not me) should flesh the idea out a bit more, perhaps connecting
it to existing systems (I remember the story about the recommender system
manipulating its users into political extremism to increase viewing time,
but I can't find a link right now).

The one thing I would like to see improved is at least some links to
prior existing work.  Christiano writes that

> (None of the concerns in this post are novel.)

but it isn't clear whether he is just summarizing things he has thought
about, which are implicit knowledge in his social web, or whether he is
summarizing existing texts. I think part I would have benefitted from a
link to Goodhart's law (or an explanation why it is something different).

1960: The Year The Singularity Was Cancelled (Scott Alexander, 2019)
--------------------------------------------------------------------

*Written for the [2019 LessWrong Review](https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review)*

[Original Post](https://www.lesswrong.com/posts/bYrF8rXFYwPqnfxTp/1960-the-year-the-singularity-was-cancelled)

I believe this is an important
[gears-level](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding)
addition to posts like [hyperbolic
growth](https://sideways-view.com/2017/10/04/hyperbolic-growth/),
[long-term growth as a sequence of exponential
modes](http://mason.gmu.edu/~rhanson/longgrow.html) and an old yudkowsky
post I am unable to find at the moment.

 I don't know how closely these
 texts are connected, but [Modeling the Human
 Trajectory](https://www.openphilanthropy.org/blog/modeling-human-trajectory)
 picks up one year later, creating two technical models: one
 stochastically fitting and extrapolating GDP growth; the other providing
 a deterministic outlook, considering labor, capital, human capital,
 technology and production (and, in one case, natural resources). Roodman
 arrives at somewhat similar conclusions, too: The industrial revolution
 was a _very_ big deal, and something happened around 1960 that has
 slowed the previous strong growth (as far as I remember, it doesn't
 provide an explicit reason for this).

A point in this post that I found especially interesting
was the speculation about the back plague being the spark
that ignited the industrial revolution. The reason given is
a good example of [slack catapulting a system out of a local
maximum](https://www.lesswrong.com/posts/GZSzMqr8hAB2dR8pk/studies-on-slack),
in this case a malthusian europe into the industrial revolution.

Interestingly, both this text and Roodman don't consider individual
intelligence as an important factor in global productivity. Despite the
well-known [Flynn-Effect](https://en.wikipedia.org/wiki/Flynn_effect)
that has mostly continued since 1960 (caveat caveat), no extraordinary
change in global productivity has occurred. This makes some sense: a
rise of less than 1 standard deviation might be appreciable, but not
groundbreaking. But the relation to artificial intelligence makes it
interesting: the purported (economic) advantage of AI systems is that
they can copy themselves, thereby making population growth not the
most constraining variable in this growth model. I don't believe this
is particularly anticipation-constraining, though: this could mean that
either the post-singularity ("singularity") world is multipolar, or the
singleton controlling everything has created many sub-agents.

I appreciate this post. I have referenced it a couple of
times in conversations. Together with the investigation
by OpenPhil it makes a solid case that [the gods of straight
lines](https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/)
have decided to throw us into [the most important century of
history](https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1).
May the [godess of everything
else](https://www.lesswrong.com/posts/MFNJ7kQttCuCXHp8P/the-goddess-of-everything-else)
be merciful with us.
